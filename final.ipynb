{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51495f46-1ef0-411a-a587-9abda8950c4d",
   "metadata": {},
   "source": [
    "# Statistical Methods Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b996b94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T08:14:33.154448900Z",
     "start_time": "2024-06-03T08:14:31.135213700Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import shapiro\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan, het_white\n",
    "from scipy.stats.mstats import winsorize\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311e0eb2-c16b-46bb-a75e-8ca86e32a18e",
   "metadata": {},
   "source": [
    "# EDA (data description, data preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750c9e88",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-06-03T08:14:33.145850700Z"
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/ov3ipo/SM_Project/main/life_expectancy.csv\")\n",
    "# remove trailing space in columns name and format display function\n",
    "pd.options.display.float_format = '{:.4f}'.format\n",
    "df = df.rename(columns=lambda x: x.strip())\n",
    "\n",
    "# overview on data statistic\n",
    "display(df.head(10))\n",
    "display(df.info())\n",
    "\n",
    "# get quantitative and qualitative data\n",
    "numeric_cols = df.drop(columns=[\"Status\", \"Country\"], axis=1).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4acd02-f2a9-4b59-bb54-99418d29b099",
   "metadata": {},
   "source": [
    "## Data description\n",
    "\n",
    "### Univariate\n",
    "\n",
    "#### Qualitative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d568f945",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,5))\n",
    "plt.subplot(1, 2, 1)\n",
    "x = df['Status'].value_counts().reset_index()\n",
    "plt.pie(x=x['count'], labels=x['Status'], autopct=\"%0.1f%%\")\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.countplot(df, x=\"Status\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb32045-08e1-4d63-8c59-8cef4045a50e",
   "metadata": {},
   "source": [
    "``` {markdown}\n",
    "Remark:\n",
    "-- Why didn't we consider the Country col in this case?\n",
    "    + Categorical variable: the \"Status\" column represent a categorical variable, which is ideal for qualitative analysis.\n",
    "    + Comparison and Analysis: using this column, one can compare other quantitative metrics (Life expectancy, GDP, etc) between developed and developing\n",
    "countries, allowing for insightful analyses on how development status affects various health and economic indicators.\n",
    "    + Simple and Interpretative: making any audience to understand easily.\n",
    "- State what you see in the chart\n",
    "- Pie chart: This chart shows the proportion of countries classified as \"Developing\" and \"Developed\". About 82.6% of the entries in the dataset are classified as\n",
    "\"Developing\" and 17.4% as \"Developed\". The large section in blue represents developing countries, while the smaller section in orange represents developed\n",
    "countries.\n",
    "- Bar chart: This chart displays the count of countries classified as \"Developing\" versus \"Developed\". It visually confirms the numbers seen in the pie chart,\n",
    "with a significantly higher count of developing countries compared to developed countries. The height of the bars indicates the count of countries in each \n",
    "ategory, reinforcing the disparity in number.\n",
    "```\n",
    "\n",
    "#### Quantitative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a70401",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 20))\n",
    "for i, col in enumerate(numeric_cols):\n",
    "    plt.subplot(10, 2, i + 1)\n",
    "    sns.histplot(df, x=col, bins=30, kde=True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393140de-6cfc-491f-b405-dc50835ae7d4",
   "metadata": {},
   "source": [
    "``` {markdown}\n",
    "Remark:\n",
    "- Adult Mortality: This histogram shows a skewed distribution with a peak around low mortality rates and a long tail extending to higher rates.\n",
    "- Income Composition of Resources: Shows a broad distribution with a peak towards higher values.\n",
    "- Schooling Years: Roughly normally distributed centered around 10-12 years of schooling.\n",
    "These histograms are useful for understanding the central tendencies and variability within the data, as well as for identifying potential outliers and skewness\n",
    "in the distributions.\n",
    "```\n",
    "\n",
    "### Bivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17c2cc1",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 20))\n",
    "index = np.argwhere(numeric_cols==\"Life expectancy\")\n",
    "for i, col in enumerate(np.delete(numeric_cols, index)):\n",
    "    plt.subplot(10, 2, i + 1)\n",
    "    sns.scatterplot(df, x=col, y=\"Life expectancy\", hue=\"Status\", legend=\"auto\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd74e0c9-b85a-41ea-8ee3-7e76d0688a11",
   "metadata": {},
   "source": [
    "``` {markdown}\n",
    "Remark:\n",
    "- State what you see in the chart\n",
    "    + Life Expectancy vs. Adult Mortality: Displays a clear negative correlation. As adult mortality increases, life expectancy decreases. This relationship\n",
    "appears to be linear and strong.\n",
    "    + Life Expectancy vs. GDP: Shows a positive correlation; countries with higher GDP tend to have higher life expectancy.\n",
    "    + Life Expectancy vs. Income Composition of Resources and Schooling: Both show strong positive correlations. Higher income composition and more years of\n",
    "schooling are associated with higher life expectancy.\n",
    "- Base on the scatter plot can you spot any variables that seem to have a linear relationship with the target variabel?\n",
    "Based on these plots, the variables that show a linear relationship with life expectancy and are most prominent include Adult Mortality, GDP, Schooling, Income\n",
    "Composition of Resources, and to a lesser extent, health expenditure and vaccination coverages. These factors appear to have a direct and significant influence on\n",
    "life expectancy, making them critical indicators for health and development studies.\n",
    "```\n",
    "\n",
    "### Overall statistic\n",
    "- Life Expectancy: Average is around 69 years but varies widely influenced by health, economic, and educational factors.\n",
    "- Health Metrics: Both adult and infant mortality rates show significant impact on life expectancy, with data suggesting that lower mortality rates correlate with\n",
    "higher life expectancy.\n",
    "- Economic Factors: GDP per capita and percentage expenditure on health significantly influence life expectancy, emphasizing the importance of economic stability\n",
    "and health investment.\n",
    "- Social Factors: Higher income resources and more years of schooling are strongly associated with higher life expectancy, highlighting the socio-economic\n",
    "foundations of health."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c46069",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "display(df.describe().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c0f755-7201-4a78-91ff-4242a56c7b4d",
   "metadata": {},
   "source": [
    "## Data preprocessing (NAs, outliers, duplicateds, label encoding)\n",
    "\n",
    "### Missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8b2ae9",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "print(\"\\nPreprocessing\\n\")\n",
    "print(df.isna().sum())\n",
    "df = df.interpolate(method='linear', limit_direction='forward')\n",
    "print(\"\\nPostprocessing\\n\")\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b645eafb-131f-4713-a78b-d41d108a1afe",
   "metadata": {},
   "source": [
    "### Duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6357f405",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "print(f\"Total duplicated values: {df.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6299669-3e36-4478-8aa3-123c34318b0a",
   "metadata": {},
   "source": [
    "### Outliers\n",
    "\n",
    "#### Detect outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdf4e0c",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 15))\n",
    "for i, col in enumerate(numeric_cols):\n",
    "    plt.subplot(10, 2, i + 1)\n",
    "    sns.boxplot(df, x=col)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# detect outliers\n",
    "def detectOutliers(data):\n",
    "    outliers_arr = []\n",
    "    for col in data.columns:\n",
    "        Q1 = data[col].quantile(0.25)\n",
    "        Q3 = data[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        outliers = data[(data[col] < lower_bound) | (data[col] > upper_bound)][col].count()\n",
    "        outliers_arr.append(outliers)\n",
    "    return pd.DataFrame(outliers_arr, index=data.columns, columns=[\"Total outliers\"])\n",
    "\n",
    "numeric_data = df.drop(columns=[\"Status\", \"Country\"], axis=1)\n",
    "outliers = detectOutliers(numeric_data)\n",
    "outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0ed8f5-f5d5-4e72-9568-cab7e1bbb285",
   "metadata": {},
   "source": [
    "``` {markdonw}\n",
    "Why we should not use variable with high outliers -> because regression can heavily be affected by these outliers, hence we should only choose those with low outliers\n",
    "Potential variable for regression of target\n",
    "- Year\n",
    "- Adult Mortality\n",
    "- Alcohol\n",
    "- BMI\n",
    "- Total expenditure\n",
    "- thinness 1-19 years\n",
    "- thinness 5-9 years\n",
    "- Income composition of resources\n",
    "- Schooling\n",
    "```\n",
    "\n",
    "#### Dealing with outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e457bc9",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# get potentital variables and variables that have outliers\n",
    "potential_var = outliers[(outliers[\"Total outliers\"] < 150)].index\n",
    "outliers_var = outliers[(outliers[\"Total outliers\"] > 0) & (outliers[\"Total outliers\"] < 150)].index\n",
    "\n",
    "# using transformation to deal with outliers\n",
    "df_outliers = df.copy()\n",
    "for col in outliers_var:\n",
    "    if col != \"Life expectancy\":\n",
    "        df_outliers[col] = np.sqrt(df_outliers[col])\n",
    "\n",
    "# detect outliers again\n",
    "display(detectOutliers(df_outliers[potential_var]))\n",
    "\n",
    "# notice that outliers of Total expenditure, Income composition of resources and schooling does not change and or increase\n",
    "for col in [\"Total expenditure\", \"Income composition of resources\", \"Schooling\"]:\n",
    "    df_outliers[col] = df[col]\n",
    "    df_outliers[col] = winsorize(df_outliers[col], limits=[0.05, 0.05])\n",
    "\n",
    "# detect outliers again\n",
    "display(detectOutliers(df_outliers[potential_var]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b557f50b",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# plot outliers\n",
    "plt.figure(figsize=(10, 15))\n",
    "for i, col in enumerate(potential_var):\n",
    "    plt.subplot(10, 2, i + 1)\n",
    "    sns.boxplot(df_outliers, x=col)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# assign new dataframe to use for regression\n",
    "df_regress = df_outliers[potential_var]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52640b11-b857-48ac-aa6f-b2ce070bdc51",
   "metadata": {},
   "source": [
    "``` {markdown}\n",
    "Remark\n",
    "- Explain reason why we should not use variables that have many outliers\n",
    "    + Skewness: Outliers can skew the probability distribution of your data\n",
    "    + Increased Variability: Outliers increase the variability in your data, which decreases statistical power\n",
    "    + Model Fit: Outliers can cause your regression model to be skewed towards these extreme values\n",
    "- Explain reason why we choose to use transformation and why is log (or anything else)\n",
    "    + Reduce Impact of Outliers: Winsorizing reduces the impact of outliers on statistical measures such as the mean and standard deviation1. It sets extreme\n",
    "outliers equal to a specified percentile of the data. This allows us to get a more accurate view of the mean and standard deviation of the dataset.\n",
    "    + Retain Observations: Unlike trimming, which removes outliers entirely, Winsorizing retains the observations that are at the extremes. It makes sense to\n",
    "Winsorize data when we want to keep the observations that are at the extremes but we don’t want to take them too literally.\n",
    "    + Avoid Data Modification: If there aren’t extreme outliers, then Winsorizing the data will only modify the smallest and largest values slightly. This is\n",
    "generally not a good idea since it means we’re just modifying data values for the sake of modifications.\n",
    "    + Investigate Outliers: Outliers can represent interesting edge cases in the data. Thus, before modifying outliers it’s a good idea to take a closer look at\n",
    "them to see what could have caused them.\n",
    "```\n",
    "\n",
    "### Comprare preprocess and postprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32555601",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# compare statistic\n",
    "display(df[potential_var].describe().T)\n",
    "display(df_regress.describe().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d239b12-5ffa-4068-a487-cbe40cea2bd0",
   "metadata": {},
   "source": [
    "``` {markdown}\n",
    "Remark:\n",
    "•\tPre-Transformation Statistics\n",
    "Before the transformation, several variables had a significant number of outliers. For instance, ‘Adult Mortality’ had 82 outliers, ‘Alcohol’ had 1 outlier, and\n",
    "‘Income composition of resources’ had 130 outliers. These outliers could potentially skew our data and impact the results of any subsequent data analysis or\n",
    "modeling.\n",
    "•\tPost-Transformation Statistics\n",
    "After applying the square root transformation and Winsorization techniques, we observed a significant reduction in the number of outliers across multiple\n",
    "variables. For example, the number of outliers in ‘Adult Mortality’ reduced from 82 to 18, and ‘Alcohol’ no longer had any outliers. This indicates that our\n",
    "transformation techniques were effective in reducing the impact of extreme values.\n",
    "```\n",
    "\n",
    "# Linear Regression Analysis\n",
    "\n",
    "## Correlation Matrix\n",
    "\n",
    "### Before dealing with outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fc95c1",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "numeric_df = df.drop(columns=[\"Country\", \"Status\"])\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(numeric_df.corr(), annot=True)\n",
    "plt.title('Heatmap Correlation')\n",
    "plt.show()\n",
    "\n",
    "# get variables that has high correlation with Life expectancy\n",
    "corrs = numeric_df.corr()['Life expectancy'].drop('Life expectancy')\n",
    "high_corr = corrs[corrs.abs() > 0.5]\n",
    "print(\"Variables have correlation larger than 0.5: \")\n",
    "high_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1ddb87-d2eb-4375-83f8-aa44f7c46c65",
   "metadata": {},
   "source": [
    "### After dealing with outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afd7495",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "numeric_df_outliers = df_outliers.drop(columns=[\"Country\", \"Status\"])\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(numeric_df_outliers.corr(), annot=True)\n",
    "plt.title('Heatmap Correlation')\n",
    "plt.show()\n",
    "\n",
    "# get variables that has high correlation with Life expectancy\n",
    "corrs = numeric_df_outliers.corr()['Life expectancy'].drop('Life expectancy')\n",
    "high_corr = corrs[corrs.abs() > 0.5]\n",
    "print(\"Variables have correlation larger than 0.5: \")\n",
    "high_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3cd38a",
   "metadata": {},
   "source": [
    "Remark\n",
    "- We have 8 variables having correlation larger than 0.5.\n",
    "- Base on the heatmap above, we have two couples \"Schooling\"-\"Income composition of resources\",and \"thinness  1-19 years\"-\"thinness 5-9 years \" have high correlation with each other, so they are multicollinearity and do not satisfy the condition of linear regression.\n",
    "- According to the outliers detection in the previous step, we can see that \"HIV/AIDS\" have more outliers than the others.\n",
    "- Therefore, This leaves us with the last 2 variables that is BMI and Adult Mortality, hence we will use them for linear regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d32484",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# update data use for regress\n",
    "df_regress = df_regress[[\"Life expectancy\", \"BMI\", \"Adult Mortality\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30480773-cc0d-42e6-abf1-f30dc7ea7991",
   "metadata": {},
   "source": [
    "``` {markdown}\n",
    "Remark\n",
    "- Base on qualitative analysis remark, outliers detection remark, and this correlation matix make the final conclusion on which variable we should use for linear regression of target\n",
    "- thiness 1-19 and 5-9 have high correlation with each other, also for income composition of resources make them not satisfy the condition of linear regression that is each variables should be independent\n",
    "- HIV/AIDS is introduced in here but it has too much outliers hence we will skip it\n",
    "- This leaves us with the last 2 variables that is BMI and Adult Mortality, hence we will use them for linear regression\n",
    "```\n",
    "\n",
    "## Least square regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013e4a64",
   "metadata": {},
   "source": [
    "### $ \\hat{Y} = Intercept + Slope*X $\n",
    "$Slope = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}                   $ <br>\n",
    "$Intercept = \\bar{y} - Slope*\\bar{x}$               <br>\n",
    "<br>\n",
    "$R^2 = \\frac{\\sum_{i=1}^n (\\hat{y}i - \\bar{y})^2}{\\sum{i=1}^n (y_i - \\bar{y})^2}$ <br>\n",
    "<br>\n",
    "The coefficient of determination, $R^2$, measures the proportion of the variance in the dependent variable that is predictable from the independent variable(s). It ranges from 0 to 1, with a higher value indicating a better fit of the model to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bd477d",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "X1 = df_regress['Adult Mortality']\n",
    "X2 = df_regress['BMI']\n",
    "y = df_regress['Life expectancy']\n",
    "\n",
    "model1 = smf.ols(formula='y ~ X1', data=df_regress).fit()\n",
    "residual1 = model1.resid\n",
    "print(model1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79e071f",
   "metadata": {},
   "source": [
    "Remark\n",
    "- The R-squared value is 0.370. This indicates that the independent variable(s), \"Adult Mortality\", in the model explain 37.0% of the variation in the dependent variable. \n",
    "- The Adjusted R-squared value is also 0.370, which suggests that the inclusion of additional predictor variables would not significantly improve the model's explanatory power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe8535b",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "model2 = smf.ols(formula='y ~ X2', data=df_regress).fit()\n",
    "residual2 = model2.resid\n",
    "print(model2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c88b623",
   "metadata": {},
   "source": [
    "Remark\n",
    "- The Adjusted R-squared value is 0.309, which indicates that the independent variables in the model explain approximately 30.9% of the variation in the dependent variable.\n",
    "\n",
    "- The R-squared value is 0.310, which is slightly higher than the Adjusted R-squared. This suggests that the inclusion of additional predictor variables may not significantly improve the model's explanatory power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e39c627",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "model3 = smf.ols(formula='y ~ X1 + X2', data=df_regress).fit()\n",
    "residual3 = model3.resid\n",
    "print(model3.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d9fc0e",
   "metadata": {},
   "source": [
    "Remark\n",
    "\n",
    "- The R-squared value is 0.502, indicating that the model explains 50.2% of the variation in the dependent variable.\n",
    "- The Adjusted R-squared value is 0.502, which is the same as the R-squared value. This suggests that the inclusion of the independent variables in the model does not significantly improve the model's explanatory power beyond what the R-squared value already captures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc780559-5947-48a9-8124-fae52779785c",
   "metadata": {},
   "source": [
    "## Check residuals for 4 assumptions\n",
    "\n",
    "### Assumption 1: Linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9d3dad",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 10))\n",
    "for i, col in enumerate(df_regress.drop(['Life expectancy'], axis=1).columns):\n",
    "    plt.subplot(2, 1, i + 1)\n",
    "    sns.scatterplot(df_regress, x=col, y=\"Life expectancy\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf4a232-759a-41bb-8ba6-c413dbde6c87",
   "metadata": {},
   "source": [
    "``` {markdonw}\n",
    "Remark\n",
    "- What does the chart conclude about the liearity?\n",
    "- What to do if this assumption is violated?\n",
    "```\n",
    "\n",
    "### Assumption 2: Independent\n",
    "\n",
    "``` {markdonw}\n",
    "Remark\n",
    "- What does the chart conclude about the linearity?\n",
    "- What to do if this assumption is violated?\n",
    "```\n",
    "\n",
    "### Assumption 3: Homoscedasticity\n",
    "\n",
    "``` {markdonw}\n",
    "Remark\n",
    "- What does the chart conclude about the linearity?\n",
    "- What to do if this assumption is violated?\n",
    "```\n",
    "\n",
    "### Assumption 4: Normality of Residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58715bda",
   "metadata": {},
   "source": [
    "# ----------------------- raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd5e5be",
   "metadata": {},
   "source": [
    "### Assumption 2: Homoscedasticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fef77a0",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def Check_Homo(residual,model_input):\n",
    "    # Homoscedasticity (White test)\n",
    "    white_test = het_white(residual, model_input.model.exog)\n",
    "    print(f'White test: Test statistic = {white_test[0]}, p-value = {white_test[1]}')\n",
    "\n",
    "    if white_test[1] <= 0.05:\n",
    "        print(\"There is significant evidence of heteroscedasticity.\\n => This implies that the assumption of homoscedasticity is violated.\")\n",
    "    else :\n",
    "        print(\"There is no significant evidence to suggest heteroscedasticity.\\n => This implies that the assumption of homoscedasticity is likely satisfied.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ac5494",
   "metadata": {},
   "source": [
    "Test for \"Adult Mortality\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9eb113",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "Check_Homo(residual1,model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba9458d",
   "metadata": {},
   "source": [
    "Test for \"BMI\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1d8b94",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "Check_Homo(residual2,model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d16e0f",
   "metadata": {},
   "source": [
    "Test for \"Adult Mortality\" and \"BMI\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b519fb",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "Check_Homo(residual3,model3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c863dad5",
   "metadata": {},
   "source": [
    "### Assumption 2: Independent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd22e15",
   "metadata": {},
   "source": [
    "\n",
    "The Durbin-Watson test is used to detect the presence of autocorrelation in the residuals of a regression model. Autocorrelation refers to the correlation between the residuals of the model, which violates the assumption of independent errors.\n",
    "\n",
    "The Durbin-Watson test statistic, denoted as \"d\", can range from 0 to 4. The interpretation of the Durbin-Watson statistic is as follows:\n",
    "\n",
    "    If d = 2, it indicates no autocorrelation.\n",
    "    If d < 1.5, it suggests positive autocorrelation.\n",
    "    If d > 2.5, it suggests negative autocorrelation.\n",
    "    If d is between 1.5 and 2.5, it generally indicates no autocorrelation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7dc882",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def Check_independent(residual):\n",
    "    dw_statistic = durbin_watson(residual)\n",
    "    print(f'Durbin-Watson statistic: {dw_statistic}')\n",
    "    if 1.5<dw_statistic<2.5 :\n",
    "        print(\"A value indicates no autocorrelation.\")\n",
    "    elif dw_statistic < 1.5:\n",
    "        print(\"A value indicates positive autocorrelation\")\n",
    "    else:\n",
    "        print(\"A value indicates negative autocorrelation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea14bb9",
   "metadata": {},
   "source": [
    "Test for \"Adult Mortality\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742d7934",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "Check_independent(residual1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0076a235",
   "metadata": {},
   "source": [
    "Test for \"BMI\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f731b9",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "Check_independent(residual2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdad02d",
   "metadata": {},
   "source": [
    "Test for \"Adult Mortality\" and \"BMI\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664b28df",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "Check_independent(residual3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c25dad",
   "metadata": {},
   "source": [
    "Assumption 4: Normality of Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d65ed14",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def check_Normality(residuals):\n",
    "    sm.qqplot(residuals, line ='45')\n",
    "    plt.title('Q-Q plot')\n",
    "    plt.show()\n",
    "    # Normality (Shapiro-Wilk test)\n",
    "    shapiro_test = shapiro(residuals)\n",
    "    print(f'Shapiro-Wilk test: Test statistic = {shapiro_test[0]}, p-value = {shapiro_test[1]}')\n",
    "    if shapiro_test[1] > 0.05:\n",
    "        print(\"There is no significant evidence to suggest that the residuals are not normally distributed.\\n => This implies that the normality assumption is likely satisfied.\")\n",
    "    else :\n",
    "        print(\"There is significant evidence that the residuals are not normally distributed.\\n => This implies that the normality assumption is violated.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecec6f0",
   "metadata": {},
   "source": [
    "Test for \"BMI\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a21f9a",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "check_Normality(residual1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4843b4d",
   "metadata": {},
   "source": [
    "Test for \"Adult Mortality\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdb3d72",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "check_Normality(residual2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3f9e34",
   "metadata": {},
   "source": [
    "Test for \"Adult Mortality\" and \"BMI\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a9d895",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "check_Normality(residual3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc647f2",
   "metadata": {},
   "source": [
    "# ----------------- raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761d2412",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# test = shapiro(df_regress[])\n",
    "# print(f'Test statistic = {test[0]}, p-value = {test[1]}')\n",
    "# if test[1] > 0.05: print(f'{col} looks normal distributed (fail to reject H0)\\n')\n",
    "# else: print(f'{col} does not normal distributed (reject H0)\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d57039c-42dd-45cb-99b5-680b90aa5869",
   "metadata": {},
   "source": [
    "``` {markdonw}\n",
    "Remark\n",
    "- What does the chart conclude about the linearity?\n",
    "- What to do if this assumption is violated?\n",
    "```\n",
    "\n",
    "## Normality check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b16bf6",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# add semicolon to prevent duplicated graph issue\n",
    "sm.qqplot(df_regress[\"Life expectancy\"], line='45');\n",
    "sm.qqplot(df_regress['BMI'], line='45');\n",
    "sm.qqplot(df_regress['Adult Mortality'], line='45');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84ed986",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# use Shapiro-Wilk test to test again\n",
    "for col in df_regress.columns:\n",
    "    test = shapiro(df_regress[col])\n",
    "    print(f'Test statistic = {test[0]}, p-value = {test[1]}')\n",
    "    if test[1] > 0.05: print(f'{col} looks normal distributed (fail to reject H0)\\n')\n",
    "    else: print(f'{col} does not normal distributed (reject H0)\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924f26dc-129d-463d-875b-e7477c77bf97",
   "metadata": {},
   "source": [
    "``` {markdown}\n",
    "Remark\n",
    "Both the QQ plot and the Shapiro-Wilk test indicate that our dataset is not normally distributed. However, as our dataset is a sample from the years 2000 to 2015\n",
    "and continues to be updated annually, the increase in size may lead it to approximate normality over time, following the Central Limit Theorem (CLT). The CLT\n",
    "suggests that with a large enough sample size, the sampling distribution of the mean will approximate a normal distribution, regardless of the initial\n",
    "distribution of the data. This principle is particularly important in inferential statistics, where the assumption of normality supports the validity of various\n",
    "statistical tests and the calculation of confidence intervals.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51362ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "data = df_regress['Life expectancy']\n",
    "plt.hist(data, color='lightgreen', ec='black', bins=100)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590fe034",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_regress['Life expectancy']\n",
    "n = len(x)\n",
    "\n",
    "y = (stats.rankdata(x)/(n+1))*2 -1\n",
    "print(np.min(x), np.max(x))\n",
    "y = np.arctanh(y)\n",
    "y = np.asarray(y)\n",
    "print(y)\n",
    "\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize = (7,4))\n",
    "ax[0].hist(x,bins=100 )\n",
    "ax[0].set_title('Original Data')\n",
    "ax[1].hist(y,bins=100 )\n",
    "ax[1].set_title('Transformed Data')\n",
    "sm.qqplot(y, line='45');\n",
    "from scipy.stats import normaltest\n",
    "stat, p = normaltest(y)\n",
    "print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "# interpret\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    " print('Sample looks Gaussian (fail to reject H0)')\n",
    "else:\n",
    " print('Sample does not look Gaussian (reject H0)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8b6620",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x1 = df_regress['BMI']\n",
    "n1 = len(x1)\n",
    "\n",
    "y1 = (stats.rankdata(x1)/(n+1))*2 -1\n",
    "\n",
    "y1 = np.arctanh(y1)\n",
    "print(np.min(y1), np.max(y1))\n",
    "print(x1)\n",
    "\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize = (7,4))\n",
    "ax[0].hist(x1,bins=100 )\n",
    "ax[0].set_title('Original Data')\n",
    "ax[1].hist(y1,bins=100 )\n",
    "ax[1].set_title('Transformed Data')\n",
    "sm.qqplot(y1, line='45');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fc92e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = df_regress['Adult Mortality']\n",
    "n2 = len(x)\n",
    "\n",
    "y2 = (stats.rankdata(x2)/(n+6))*2 -1\n",
    "\n",
    "y2 = np.arctanh(y2)\n",
    "print(np.min(y2), np.max(y2))\n",
    "\n",
    "\n",
    "print(x2)\n",
    "\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize = (7,4))\n",
    "ax[0].hist(x2,bins=100 )\n",
    "ax[0].set_title('Original Data')\n",
    "ax[1].hist(y2,bins=100 )\n",
    "ax[1].set_title('Transformed Data')\n",
    "sm.qqplot(y2, line='45');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a5761d",
   "metadata": {},
   "source": [
    "## Construct confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207d8754530bf24e",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "confidence_level = 0.95\n",
    "def CI_mean(data, name):\n",
    "\n",
    "  data1 = data.values\n",
    "  data_mean = np.mean(data1)\n",
    "  data_std = np.std(data1, ddof=1)\n",
    "  n = len(data1)\n",
    "  data_interval = stats.t.interval(confidence_level, df = n-1, loc = data_mean, scale = data_std/np.sqrt(n))\n",
    "  print(f\"95% Confident that the mean of {name} lie between {data_interval}\")\n",
    "CI_mean(df_regress['BMI'], \"BMI\")\n",
    "CI_mean(df_regress['Adult Mortality'], 'Adult Mortality')\n",
    "CI_mean(df_regress['Life expectancy'], 'Life expectancy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639aa381",
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_level = 0.95\n",
    "def CI_var_std(data, name):\n",
    "  data_var = np.var(data, ddof = 1)\n",
    "  data_std = np.std(data, ddof =1)\n",
    "  n = len(data) - 1\n",
    "  chi2 = stats.chi2.ppf((1 + confidence_level)/2, n)\n",
    "  CI_var = (n * data_var / chi2, n * data_var / stats.chi2.ppf((1-confidence_level)/2,n))\n",
    "  CI_std = np.sqrt(CI_var)\n",
    "  print(f\"95% Confident that Variance of {name} lie between {CI_var} :\")\n",
    "  print(f\"95% Confident that Standard Deviation of {name} lie between {CI_std} :\")\n",
    "CI_var_std(df_regress['BMI'], 'BMI')\n",
    "CI_var_std(df_regress['Adult Mortality'], 'Adult Mortality')\n",
    "CI_var_std(df_regress['Life expectancy'],'Life expectancy' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a145544",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confidence interval for proportion of Life expectancy higher than 60\n",
    "LElarger60 = df_regress[df_regress['Life expectancy'] >= 60]\n",
    "confidence_level=0.95\n",
    "def CI_proportion(data, sample, name):\n",
    "    n = len(data)\n",
    "    s = len(LElarger60)\n",
    "    p = s / n\n",
    "\n",
    "    se = np.sqrt(p * (1 - p) / n)\n",
    "    z = stats.norm.ppf(1 - (1 - confidence_level) / 2)\n",
    "    lower_bound = p - z * se\n",
    "    upper_bound = p + z * se\n",
    "    #return lower_bound, upper_bound\n",
    "    print(f\" 95% Confident that proportion of {name} is between ({lower_bound}, {upper_bound})\")\n",
    "\n",
    "CI_proportion(df_regress['Life expectancy'], LElarger60, 'People whose Life expectancy is greater than 60')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8d499d",
   "metadata": {},
   "source": [
    "## Perform hypothesis testing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
